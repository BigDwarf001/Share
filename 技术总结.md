# 1. 单机FIO测试
## 1.1 配置FS环境
略~
## 1.2 配置 Linux NVMe over Fabrics 主机  
安装nvme可通过 ` yum search nvme  ` 找到对应的nvme版本，然后 `yum install nvme-cli.*`
Discovery: `nvme discover -t tcp -a 192.168.10.194 -s 4420`
Connect: `nvme connect -t tcp -n "nqn.2016-06.io.spdk:cnode1" -a 192.168.10.194 -s 4420`
Disconnect: `nvme disconnect -n "nqn.2016-06.io.spdk:cnode1"`
下面显示了用于加载驱动程序nvme-tcp的命令 `modprobe nvme-tcp`

## 1.3 FIO通过配置文件运行
除了命令行直接执行命令外，也可以通过写配置到 **xxx.fio**文件中，每次只用修改配置即可，使用更方便些，执行方式为 `fio xxx.fio`
比如测试10.168.1.194控制器的FIO，那就将偶数编号的卷写入**xxx.fio**文件中。
卷的编号 `nvme list`
~~~
# fio配置文件举例
[global]
bs=512		# 单次io的块文件大小
ioengine=libaio		# Linux本地异步I/O，libaio工作的时候需要文件direct方式打开
userspace_reap		# 提高异步IO收割的速度
rw=randrw		# 测试随机写和读的I/O
rwmixwrite=20		# 在混合读写的模式下，写占20%
time_based
runtime=180		# 基本单位秒
direct=1		# 如果为真，则使用非缓冲 I/O
group_reporting		# 关于显示结果的，汇总每个进程的信息
randrepeat=0
norandommap
ramp_time=6
iodepth=16		# 针对文件保持运行的 I/O 单元数。
iodepth_batch=8		# 一次提交的 I/O 数量
iodepth_low=8		# 指示何时再次开始填充队列的低水位线
iodepth_batch_complete=8		# 这定义了一次检索多少个 IO
exitall
[test]
filename=/dev/nvdisk0		# 测试文件名称，通常选择需要测试的盘的data目录
numjobs=1
~~~
## 1.4 双机+双fio测试
tips：
- 每次操作前首先将`/opt/leadstor/lsfs/`目录下的`lsfstool、afa_tgt`文件备份。
- 在3.75上执行`ipmitool -I lanplus -H 192.168.1.127 -U ADMIN -P ADMIN chassis power on/off/cycle`可以关掉125的机器（或者shutdown）
- 192.168.1.127是192.168.1.125的管理地址

客户端连接服务器（双机模式）之后，每一步连接之后创执行 `nvme list `查看对应的映射盘号，不然后续两个重复的无法分辨；或者全部连接之后，执行 `nvme list-subsys`查看详细信息。![在这里插入图片描述](https://img-blog.csdnimg.cn/c69560d134e944209eebaa459925bf14.png)![在这里插入图片描述](https://img-blog.csdnimg.cn/59ce84152c574f9eb30d18b9dd4530dd.png)
查看连接的nvme状态
![在这里插入图片描述](https://img-blog.csdnimg.cn/d2105c694a9840688649b064809ae696.png)
可以看到最下面四行即为我们连接的卷组映射，/dev/nvme24n1和/dev/nvme26n1、/dev/nvme25n1和/dev/nvme27n1是重复的，执行双机双fio测试就要将四个卷组映射挑选出两个（/dev/nvme25n1、/dev/nvme26n1）执行。

- 遇到问题
	- 运行错误显示`fio: looks like your file system does not support direct=1/buffered=0`；改为direct=0
	- 设备没有空余空间`fio: io_u error on file /dev/nvme25n1: No space left on device: write offset=16849829888, buflen=32768`；~~size改为5GB~~ 
	- 上一条是因为NVMF未启动导致写测试并没有真正写到磁盘里，因此先删除dev里面生成的文件（Linux特性，将磁盘读写当成文件，所以没有磁盘设备时会新建一个文件）
	- 客户端连接、测试之后，并没有及时关闭连接造成客户端崩溃；及时`nvme disconnect-all`
	- 关闭客户端`shutdown`，在192.1683.75上执行`ipmitool -I lanplus -H 192.168.1.127 -U ADMIN -P ADMIN chassis power on/off/cycle`可以关掉125的机器
	- 第二天显示fs系统创建异常；在LSMC停止的情况重启数据库 
		- ` pcsd LSMC`
		- `/opt/leadstor/lsmc/bin/db_setup.py reset` 
		- `pcse LSMC`
		- `tl #实时监控fs启动`
## 1.5 fio测试
fio测试写验证（注意size不超过卷的大小）配置文件
~~~
rw=write
verify=crc32c
verify_fatal=1
verify_dump=1
verify_backlog=1
~~~
vim配置，其他见[链接](https://www.cnblogs.com/ruan-kai/p/14924786.html)
~~~
<1> zf(Fold creation)–创建折叠(注意在.vimrc中设置set foldmethod=marker)
zf56G，创建从当前行起到56行的代码折叠；
10zf或10zf+或zf10↓，创建从当前行起到后10行的代码折叠。
10zf-或zf10↑，创建从当前行起到之前10行的代码折叠。
在括号处zf%，创建从当前行起到对应的匹配的括号上去（（），{}，[]，<>等）。

zr 打开被折叠的代码
zm 折叠打开的代码

zD 循环删除 (Delete) 光标下的折叠，即嵌套删除折叠。仅当 ‘foldmethod’ 设为 “manual” 或 “marker” 时有效。
zE 除去 (Eliminate) 窗口里“所有”的折叠。仅当 ‘foldmethod’ 设为 “manual” 或 “marker” 时有效。
~~~

# 2. 优化代码
## 2.1 程序运行细节
检查代码内存泄漏：
~~~
cp /root/workspace/afa-spdk/module/bdev/lsfsvol/lib_ddn_test.c /root/workspace/lsfs-main/test/devtest
cd /root/workspace/lsfs-main/test/devtest
make test
~~~
运行asan
~~~
LD_PRELOAD="/lib64/libasan.so.5" ./lib_ddn_test

gdb --args ./afa_tgt
set exec-wrapper env LD_PRELOAD=/lib64/libasan.so.5

make test_asan
/root/workspace/lsfs-main/build/test_asan/devtest
~~~
在gdb中运行可查看内存泄漏

# 3. 性能测试

在spdk启动之前首先要创建池保证性能测试的环境适合，所以第一步进行FS系统的创建：

①更换文件(195和194更换文件步骤相同，以194为例)

~~~
# root@192.168.1.194
cd /opt/leadstor/lsfs/
rm -f afa_tgt
cp afa_tgt.0808 afa_tgt
rm -f lsfstool
cp lsfstool.0808 lsfstool
~~~

②重置数据库（194和195机器数据库同一份，重启一次即可）

~~~
# root@192.168.1.194
/opt/leadstor/lsmc/bin/db_setup.py reset
~~~

③启动FS系统

~~~
# root@192.168.1.194
pcse LSMC
# 或者pcsr重启
~~~

④登录`192.168.1.193`创建盘组和池

**问题1**、出现磁盘名称收集中，应该是连续的磁盘读写负载太大；执行lsfstool的时候加-d参数   `lsfstool Create -e 2 -d` ，这是对盘做一次trim操作，可以提高盘性能

⑤关闭FS系统

~~~
# root@192.168.1.194
pcsd LSMC
~~~

#
**性能测试分为单机、双机两种，在准备性能测试环境的过程中最好要检测每个脚本的正确性、是否和运行的服务器配置相对应**

## 3.1 spdk启动

首先在服务器（194和195都要创建一份，单机测试创建一个就行）上创建一个自己的目录，把`/root/sqh`目录复制到自己的目录中，例如：`cp /root/sqh /root/ws`

`cd /root/ws`

首先修改`gdblsfs.sh`文件，如下`cd /root/ws/`改为自己的新建目录，然后设置数据库（0，0）或（1，3）

然后设置CPU核心，对应自己机器上的核心数。CPU核心数解释：0x0FFF000000000000展开为0000 1111 1111 1111 0000 0000 0000 0000 ...每一位二进制数代表一个CPU，所以0FFF000000000000表示了64个CPU，二进制数为1表示CPU可用，所有以上可用12个CPU

~~~
#! /bin/bash
#清理环境
echo 0 > /proc/sys/vm/nr_hugepages
echo 0 > /proc/sys/vm/nr_hugepages

#重置硬盘
cd /root/ws/
./hugepage.sh
./lsfstool Create -e 2
#./cp_dump -e
#设置 数据库 0 0 1 3代表node0状态0，node1状态3（停止）
#redis-cli -p 6379 hmset node 0 0 1 3
redis-cli -p 6379 hmset node 0 0 1 0
redis-cli -p 6379 keys volume*|xargs redis-cli -p 6379 DEL
redis-cli -p 6379 keys snapshot*|xargs redis-cli -p 6379 DEL
redis-cli -p 6379 keys fs_gns_sync*|xargs redis-cli -p 6379 DEL
redis-cli -p 6379 keys fs_sr*|xargs redis-cli -p 6379 DEL
......
# 设置CPU核心数
gdb --args $(pwd)/afa_tgt -c $(pwd)/spdk_config.json.empty -r 10.0.0.110:8888 -m 0x0FFF000000000000;
~~~

双机或单机运行`gdblsfs.sh`，结果

![](D:\Work\Note\image\1.png)

然后出现gdb命令输入r继续，接着会弹出日志记录，**日志记录一直运行不能中断**

**问题2**、日志记录可能会出现`spdk_jsonrpc_server_listen: *ERROR*: could not bind JSON-RPC server: Address already in use`
![](D:\Work\Note\image\2.png)

~~~
# 首先查找端口进程，然后kill
lsof -i:8888
# COMMAND    PID  USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
# reactor_4  5887 root  159u  IPv4 119715      0t0  TCP afa_primary:ddi-tcp-1 (LISTEN)
kill -9 5887
~~~

**问题3**、单机运行`gdblsfs.sh`只能是194（master）上机器运行，如果想切换195（slave）单机运行可以在现在的master机器上输入命令将master机器设置为195（但是一般单机测试只做一台机器就可以）

~~~
# root@192.168.1.194
pcs cluster standby
~~~

**问题4**、双机运行`gdblsfs.sh` 的脚本是不相同的。

## 3.2 bdev设备（双机或单机）

同样的先修改代码，修改地址 `/root/ws/s1.sh`

`lsfs_create_js_malloc_buf 60 -p 0` 方法是lsfs创建js缓冲区，后面60为创建60G的缓冲区如果空间不足，可以选择创建60->40的缓冲区空间大小

`nvmf_create_transport -t rdma`命令参数-t rdma/tcp为SPDK的通信方式，依据服务器情况自行选择（一般SPDK自带的为tcp）

`nvmf_subsystem_add_listener nqn.2016-06.io.spdk:jsdisk -t rdma -s 4420 -a 192.168.110.194` 命令参数-a 地址修改为自己服务器的网卡地址（找一个空闲的网卡端口即可）

~~~
#!/usr/bin/bash
cd /root/ws/
# lsfs_create_js_malloc_buf方法是lsfs创建js缓冲区，后面60为创建60G的缓冲区
#./rpc.py -s 10.0.0.110 -p 8888 lsfs_create_js_malloc_buf 40 -f /dev/dax0.0
./rpc.py -s 10.0.0.110 -p 8888 lsfs_create_js_malloc_buf 60 -p 0
./rpc.py -s 10.0.0.110 -p 8888 nvmf_create_transport -t rdma -u 16384 -p 8 -c 8192

./rpc.py -s 10.0.0.110 -p 8888 lsfs_create_js_malloc_disk  JsMallocDisk -d 0
./rpc.py -s 10.0.0.110 -p 8888 nvmf_create_subsystem nqn.2016-06.io.spdk:jsdisk -s SN000001 -d TAR-8
./rpc.py -s 10.0.0.110 -p 8888 nvmf_subsystem_allow_any_host nqn.2016-06.io.spdk:jsdisk
./rpc.py -s 10.0.0.110 -p 8888 nvmf_subsystem_add_ns nqn.2016-06.io.spdk:jsdisk JsMallocDisk
./rpc.py -s 10.0.0.110 -p 8888 nvmf_subsystem_add_listener nqn.2016-06.io.spdk:jsdisk -t rdma -s 4420 -a 192.168.110.194
~~~

最后单机或双机运行
![](D:\Work\Note\image\5.png)
**问题5**、不出现上述结果图片，原因是配置不正确。

**注意：双机运行时，每一台服务器的配置文件（s1.sh）都要修改为与服务器对应的地址、命令参数-t、IP地址-a**

## 3.3 互相连接（双机）

 `/root/ws/s2.sh`194和195服务器上的同步卷互相连接，以便进行同步操作

 ~~~
# 连接本台服务器控制的卷（另一台服务器正好相反）
nvme disconnect -n nqn.2016-06.io.spdk:jsdisk
nvme disconnect -n nqn.2016-06.io.spdk:jsdisk2
nvme disconnect -n nqn.2016-06.io.spdk:jsdisk3
nvme disconnect -n nqn.2016-06.io.spdk:jsdisk4
# 连接另一台服务器上控制的卷
nvme discover -t rdma -a 192.168.110.195 -s 4420
nvme connect -t rdma -n nqn.2016-06.io.spdk:jsdisk -a 192.168.110.195 -s 4420
nvme connect -t rdma -n nqn.2016-06.io.spdk:jsdisk2 -a 192.168.110.195 -s 4420
nvme connect -t rdma -n nqn.2016-06.io.spdk:jsdisk3 -a 192.168.110.195 -s 4420
nvme connect -t rdma -n nqn.2016-06.io.spdk:jsdisk4 -a 192.168.110.195 -s 4420
nvme list
 ~~~

 **问题6**、出现错误是因为配置文件改成服务器对应的地址

 **单机不不需要互相连接**

## 3.4 FS启动

`/root/ws/s3.sh `
**类似的，单机运行FS启动不需要数据同步这一步**

~~~
#!/usr/bin/bash
# 数据同步
./rpc.py -s 10.0.0.110 -p 8888 lsfs_setup_js_virtual_disk -d1 /dev/nvme22n1 -d2 /dev/nvme23n1 -d3 /dev/nvme24n1 -d4 /dev/nvme25n1

# FS系统启动
./rpc.py -s 10.0.0.110 -p 8888 lsfs_start
~~~

![](D:\Work\Note\image\6.png)

## 3.5 虚拟主机（双机或单机）

首先检查`blk_lsfs_vm_tgt.sh`，需要注意的是`--cpumask 0x0001000000000000 vhost.11 000002LEADST0c20` CPUMASK的值必须保证和 `gdblsfs.sh` 相对应，也就是CPUMASK对应的CPU编号必须是可用的；

然后 `vhost.11 000002LEADST0c20` 后面的 `000002` 是vhost编号，`0c20` 代表了 `c2` 对应10进制的服务器地址 `194` ，**这一个值两个服务器保持一致**，都设置为 194， 原因是194和195共用一个数据库。

~~~
./rpc.py -s 10.0.0.110 -p 8888 vhost_create_blk_controller --cpumask 0x0001000000000000 vhost.11 000002LEADST0c20
./rpc.py -s 10.0.0.110 -p 8888 vhost_create_blk_controller --cpumask 0x0002000000000000 vhost.12 000004LEADST0c20
./rpc.py -s 10.0.0.110 -p 8888 vhost_create_blk_controller --cpumask 0x0004000000000000 vhost.13 000006LEADST0c20
./rpc.py -s 10.0.0.110 -p 8888 vhost_create_blk_controller --cpumask 0x0008000000000000 vhost.14 000008LEADST0c20
./rpc.py -s 10.0.0.110 -p 8888 vhost_create_blk_controller --cpumask 0x0010000000000000 vhost.15 00000aLEADST0c20
./rpc.py -s 10.0.0.110 -p 8888 vhost_create_blk_controller --cpumask 0x0020000000000000 vhost.16 00000cLEADST0c20
./rpc.py -s 10.0.0.110 -p 8888 vhost_create_blk_controller --cpumask 0x0040000000000000 vhost.17 00000eLEADST0c20
./rpc.py -s 10.0.0.110 -p 8888 vhost_create_blk_controller --cpumask 0x0080000000000000 vhost.18 000010LEADST0c20
~~~

### 方法一（通过创建虚拟主机的方法）：

a. 启动虚拟机，194、195启动vm脚本都在`screen -d -m /var/nas/vm_start.sh` 或者 `screen -d -m vm_start.sh`

b. 创建vhost，194、195创建vhost脚本都在`/opt/leadstor/lsfs/blk_lsfs_vm_tgt.sh` 或者 `./blk_lsfs_vm_tgt.sh`

c. 导入卷，194、195均需执行 `telnet 127.0.0.1 9977`

~~~
登录194后通过ssh 10.0.0.112访问194vm
登录195后通过ssh 10.0.0.113访问195vm
~~~

具体见：[194qemu-vm测试说明](https://doc.weixin.qq.com/doc/w3_ADwAawatAEkJoZJPzG5Szubs0eTMf?scode=AM8AmQc7AAgH8M18tPAWMAIAYjANg)

**问题7**、执行`blk_lsfs_vm_tgt.sh `显示没有设备，原因 `gdblsfs.sh` 的数据库状态改成单机或双机。

### 方法二（通过本机有线网连接）：

映射卷，导入spdk `/root/ws/blk_lsfs_vm_tgt.sh`

`/var/nas/orgin_vm_start.sh &` 后面跟&可以在后台运行，同时还要配置ip地址（ipconfig查看本机ip地址）

~~~
#!/usr/bin/env bash

source /opt/leadstor/lsmc/bin/conf/common.sh

......

# Start here
main() {
    # 设置为本机地址
    export DISPLAY="192.168.2.183:0.0"
    start_vm
}
main
~~~



## 3.6 虚拟主机创建完成之后（双机或单机）

在194和195服务器上`telent 127.0.0.1 9977`打开`qemu`，然后配置`vhost`。注意修改 `path=/root/ws/vhost.11`

~~~
chardev-add socket,id=spdk_vhost_blk1,path=/root/ws/vhost.11
device_add vhost-user-blk-pci,id=blk1,chardev=spdk_vhost_blk1,num-queues=2,queue-size=1024

chardev-add socket,id=spdk_vhost_blk2,path=/root/ws/vhost.12
device_add vhost-user-blk-pci,id=blk2,chardev=spdk_vhost_blk2,num-queues=2,queue-size=1024

chardev-add socket,id=spdk_vhost_blk3,path=/root/ws/vhost.13
device_add vhost-user-blk-pci,id=blk3,chardev=spdk_vhost_blk3,num-queues=2,queue-size=1024

chardev-add socket,id=spdk_vhost_blk4,path=/root/ws/vhost.14
device_add vhost-user-blk-pci,id=blk4,chardev=spdk_vhost_blk4,num-queues=2,queue-size=1024

chardev-add socket,id=spdk_vhost_blk5,path=/root/ws/vhost.15
device_add vhost-user-blk-pci,id=blk5,chardev=spdk_vhost_blk5,num-queues=2,queue-size=1024

chardev-add socket,id=spdk_vhost_blk6,path=/root/ws/vhost.16
device_add vhost-user-blk-pci,id=blk6,chardev=spdk_vhost_blk6,num-queues=2,queue-size=1024

chardev-add socket,id=spdk_vhost_blk7,path=/root/ws/vhost.17
device_add vhost-user-blk-pci,id=blk7,chardev=spdk_vhost_blk7,num-queues=2,queue-size=1024

chardev-add socket,id=spdk_vhost_blk8,path=/root/ws/vhost.18
device_add vhost-user-blk-pci,id=blk8,chardev=spdk_vhost_blk8,num-queues=2,queue-size=1024
~~~

![](D:\Work\Note\image\7.png)
在虚拟机上输入`lsblk`查看194服务器上连接的卷

之后在虚拟主机上执行fio文件，记录ddn性能测试的数据，例如ipos。最后关闭虚拟主机`shutdown now`

**fio读操作之前确保磁盘里有数据，一般读之前顺序写一遍**

**问题8**、执行FIO时显示
![](D:\Work\Note\image\8.png)
是因为磁盘空间不足，需要重启

**问题9**、执行FIO时宕机，可能的原因是iodepth太大导致的rdma链路问题



## 附1、FS正常关机

单机或双机启动fs之后，在虚拟机内部执行

~~~shell
mkfs.ext4 /dev/vda  #格式化
mount /dev/vda /mnt/vda/   #挂载
#/root/lmf/img这个文件必须存在，可以touch img
cp /root/lmf/img /mnt/vda/
dd if=/dev/urandom of=/mnt/vda/img bs=16k count=10000   #写数据

cd /root/lmf/
#计算文件值并保存
./md5File.sh
~~~

然后在宿主机上执行

~~~shell
#设置正常关机参数
./rpc.py -s 10.0.0.110 -p 8888 lsfs_js_flush
#关机
redis-cli hset node 0 2
~~~

开机之后打开虚拟机执行` ./md5File.sh -r `校验是否正常关机

~~~shell
#!/bin/bash
#md5File.sh

path=(fio.sh redisInit.sh)
md5=()

computeMd5(){
    i=0
    while(($i<${#path[*]}))
    do
        md5sum=(`md5sum ${path[i]}`)
	    md5[i]=${md5sum[0]}
        ((i++))
    done

    echo ${path[*]}
    echo ${md5[*]}
}

diffMd5(){
    echo verify start
    md5Old=(`cat md5.backup`)

	i=0
	while(($i<${#path[*]}))
	do
	    if [ ${md5[i]} != ${md5Old[i]} ]; then
		    echo diff:${path[i]}
		fi
	    ((i++))
	done
	echo verify done
}

computeMd5

while getopts :r args
do
    case $args in
	    r)
			diffMd5;;
		*)
		    echo unkonwn;;
	esac
done

touch md5.backup
echo ${md5[*]} > md5.backup
~~~

# 4 盘的管理和划分

## 4.1 redis数据库

数据库配置文件：`/etc/redis.conf`

数据库日志：`/var/log/redis/redis.log`

~~~shell
# 连接到主机为127.0.0.1，端口为6379，密码为mypass的redis服务
redis-cli -h 127.0.0.1 -p 6379 -a "mypass"
~~~

Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合)。

String（字符串）：

- string 是 redis 最基本的类型，你可以理解成与 Memcached 一模一样的类型，一个 key 对应一个 value。
- string 类型是二进制安全的。意思是 redis 的 string 可以包含任何数据，比如jpg图片或者序列化的对象。
- string 类型是 Redis 最基本的数据类型，string 类型的值最大能存储 512MB。

Hash（哈希）：Redis hash 是一个 string 类型的 field 和 value 的映射表，hash 特别适合用于存储对象。

list（列表）：Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。

set（集合）：Redis 的 Set 是 string 类型的无序集合。集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是 O(1)。

zset：和 set 一样也是string类型元素的集合，且不允许重复的成员。每个元素都会关联一个double类型的分数，redis正是通过分数来为集合中的成员进行从小到大的排序。

### 4.1.1 redis键

Redis Keys 命令用于查找所有符合给定模式 pattern 的 key 。

~~~shell
redis-cli keys pattern 
redis-cli set keyname value
redis-cli DEL keyname
~~~

### 4.1.2 redis哈希

~~~shell
# 获取存储在哈希表中指定字段的值
redis-cli hget KEY_NAME FIELD
redis-cli hset KEY_NAME FIELD value
# multiply
redis-cli hmget KEY_NAME FIELD1 VALUE1...FIELDn VALUEn
redis-cli hmset KEY_NAME FIELD1 VALUE1...FIELDn VALUEn
~~~

raid:DiskGroup
